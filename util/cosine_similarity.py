# -*- coding: utf-8 -*-
"""Cosine Similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14DJkJRj-SfCOu82o7qd18JJUnP5iCcdn
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from pprint import pprint

from sklearn.model_selection import cross_val_predict
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

from sklearn.model_selection import KFold
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import numpy as np
import pandas as pd

from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder 
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
import numpy as np

from sklearn.metrics import precision_recall_fscore_support as score
from wordcloud import WordCloud,STOPWORDS
import pandas as pd
import numpy as np

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem.wordnet import WordNetLemmatizer

import string

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.models import Sequential
import os
import matplotlib.pyplot as plt

from numpy import array
from numpy import asarray
from numpy import zeros
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.models import load_model
from keras.layers import Embedding


from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import preprocessing
import re
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

le = LabelEncoder() 

#data = pd.read_csv('Dataset.csv')
label_encoder = preprocessing.LabelEncoder()
student_answer = 'By letting it sit in a dish for a day'
ref_answers = 'The water was evaporated, leaving the salt'
ps = PorterStemmer()
#answers_arr = answers.to_numpy()
ex_sentence = student_answer
ref_sentance = ref_answers
stop_words = set(stopwords.words("english"))

def clean(s):
  out = word_tokenize(s)
  ans = ' '.join([''.join(ps.stem(word)) for word in out if word not in stop_words])
  return ans
filtered_answers = [clean(ex_sentence) for s in student_answer]


def clean(s):
  out = word_tokenize(s)
  ans = ' '.join([''.join(ps.stem(word)) for word in out if word not in stop_words])
  return ans
filtered_answers1 = [clean(ref_sentance) for s in ref_answers]

answers= filtered_answers+filtered_answers1

l=len(filtered_answers)

test_sent = answers
cv = TfidfVectorizer()
tfidf = cv.fit_transform(test_sent)
students_answers = tfidf[:l]
model_answers = tfidf[l:]

cosine_similarity(students_answers[1], model_answers[1])

threshold = 0.01
predictions = []
for i in range(l):
  predictions.append(*(cosine_similarity(students_answers[i], model_answers[i]) > threshold)[0])


predictions = [1 if predictions[i] == True else 0 for i in range(len(predictions))]